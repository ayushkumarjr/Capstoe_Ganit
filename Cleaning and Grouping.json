{
	"jobConfig": {
		"name": "Cleaning and Grouping",
		"description": "",
		"role": "arn:aws:iam::211125595857:role/Glue_key",
		"command": "pythonshell",
		"version": "3.0",
		"runtime": null,
		"workerType": null,
		"numberOfWorkers": null,
		"maxCapacity": 1,
		"maxRetries": 0,
		"timeout": 2880,
		"maxConcurrentRuns": 1,
		"security": "none",
		"scriptName": "Cleaning and Grouping.py",
		"scriptLocation": "s3://aws-glue-assets-211125595857-ap-south-1/scripts/",
		"language": "python-3.9",
		"spark": false,
		"jobParameters": [],
		"tags": [],
		"jobMode": "DEVELOPER_MODE",
		"createdOn": "2024-08-10T08:31:05.833Z",
		"developerMode": true,
		"connectionsList": [],
		"temporaryDirectory": "s3://aws-glue-assets-211125595857-ap-south-1/temporary/",
		"glueHiveMetastore": true,
		"etlAutoTuning": false,
		"observabilityMetrics": false,
		"pythonShellPrebuiltLibraryOption": "analytics",
		"flexExecution": false,
		"minFlexWorkers": null,
		"sourceControlDetails": {
			"Provider": "GITHUB",
			"Repository": "Capstoe_Ganit",
			"Branch": "main",
			"Folder": "Cleaning code"
		},
		"maintenanceWindow": null,
		"pythonPath": null
	},
	"hasBeenSaved": false,
	"usageProfileName": null,
	"script": "import boto3\r\nimport pandas as pd\r\nfrom io import StringIO\r\n\r\n# Initialize a session using Amazon S3\r\ns3 = boto3.client('s3')\r\n\r\n# Define the S3 bucket and object (file) key\r\nbucket_name = 'fetching-historical-data'\r\n\r\n# List of file keys\r\nfile_keys = [\r\n    '1-3000.csv','12001-15635.csv','15637-18637.csv','18637-21636.csv',\r\n    '21637:24637.csv','24637:27637.csv','27637:31276.csv','3001-6000.csv',\r\n    '31276-36276.csv','36277:41276.csv','41277:46912.csv','6001-9000.csv',\r\n    '9001-12000.csv'\r\n]\r\n\r\n# Loop through the file keys and read the CSV files\r\nfor i, file_key in enumerate(file_keys):\r\n    try:\r\n        response = s3.get_object(Bucket=bucket_name, Key=file_key)\r\n        csv_content = response['Body'].read().decode('utf-8')\r\n        data_io = StringIO(csv_content)\r\n        globals()[f'df{i+1}'] = pd.read_csv(data_io)\r\n        print(globals()[f'df{i+1}'])\r\n    except Exception as e:\r\n        print(f\"Error reading {file_key}: {e}\")\r\n        \r\n        \r\n# Specify the date format explicitly if you know it\r\ndate_format = '%Y-%m-%d'  # Update this format to match your date format\r\n\r\n# Initialize empty lists to store combined data for each year range\r\ncombined_df_2006_2011 = []\r\ncombined_df_2012_2015 = []\r\ncombined_df_2016_2019 = []\r\ncombined_df_2020_2022 = []\r\ncombined_df_2023_2024 = []\r\n\r\nfor i in range(1, 14):\r\n    df_i = eval(f'df{i}').copy()  # Create a copy of the data frame\r\n    \r\n    if 'date' not in df_i.columns:\r\n        print(f\"'date' column missing in df{i}\")\r\n        continue\r\n    \r\n    df_i['date'] = pd.to_datetime(df_i['date'])  # Specify the date format\r\n    df_i['year'] = df_i['date'].dt.year\r\n\r\n    filtered_df1_2006_2011 = df_i[(df_i['year'] >= 2006) & (df_i['year'] <= 2011)]\r\n    combined_df_2006_2011.append(filtered_df1_2006_2011)\r\n\r\n    filtered_df1_2012_2015 = df_i[(df_i['year'] >= 2012) & (df_i['year'] <= 2015)]\r\n    combined_df_2012_2015.append(filtered_df1_2012_2015)\r\n\r\n    filtered_df1_2016_2019 = df_i[(df_i['year'] >= 2016) & (df_i['year'] <= 2019)]\r\n    combined_df_2016_2019.append(filtered_df1_2016_2019)\r\n\r\n    filtered_df1_2020_2022 = df_i[(df_i['year'] >= 2020) & (df_i['year'] <= 2022)]\r\n    combined_df_2020_2022.append(filtered_df1_2020_2022)\r\n\r\n    filtered_df1_2023_2024 = df_i[(df_i['year'] >= 2023) & (df_i['year'] <= 2024)]\r\n    combined_df_2023_2024.append(filtered_df1_2023_2024)\r\n\r\n# Concatenate all data frames for each year range\r\ncombined_df_2006_2011 = pd.concat(combined_df_2006_2011, ignore_index=True)\r\ncombined_df_2012_2015 = pd.concat(combined_df_2012_2015, ignore_index=True)\r\ncombined_df_2016_2019 = pd.concat(combined_df_2016_2019, ignore_index=True)\r\ncombined_df_2020_2022 = pd.concat(combined_df_2020_2022, ignore_index=True)\r\ncombined_df_2023_2024 = pd.concat(combined_df_2023_2024, ignore_index=True)\r\n\r\n\r\ndef clean_text(scheme):\r\n    if 'Mutual Fund' in scheme:\r\n        return 'Mutual Fund'\r\n    elif 'Open Ended' in scheme:\r\n        return 'Open Ended Scheme'\r\n    elif 'Close Ended' in scheme:\r\n        return 'Close Ended Scheme'\r\n    elif 'Interval Fund' in scheme:\r\n        return 'Interval Fund'\r\n    elif 'Debt' in scheme or 'FMP' in scheme:\r\n        return 'Debt Scheme'\r\n    elif 'Hybrid' in scheme:\r\n        return 'Hybrid Scheme'\r\n    elif 'Equity' in scheme:\r\n        return 'Equity Scheme'\r\n    elif 'Index Fund' in scheme:\r\n        return 'Index Fund'\r\n    elif 'Liquid' in scheme or 'Ultra Short Duration' in scheme:\r\n        return 'Liquid/Ultra Short Duration Fund'\r\n    elif 'Plan' in scheme or 'Days' in scheme:\r\n        return 'Plan/Duration'\r\n    else:\r\n        return 'Uncategorized'\r\n\r\n\r\ndef categorize4(scheme):\r\n    if 'EQUITY SCHEME' in scheme:\r\n        return 'EQUITY SCHEME'\r\n    elif 'DEBT SCHEME' in scheme:\r\n        return 'DEBT SCHEME'\r\n    elif 'HYBRID SCHEME' in scheme:\r\n        return 'HYBRID SCHEMES'\r\n    elif 'SOLUTION ORIENTED SCHEME - RETIREMENT FUND' in scheme:\r\n        return 'SOLUTION ORIENTED SCHEME RETIREMENT'\r\n    elif 'SOLUTION ORIENTED SCHEME - CHILDRENâ€™S FUND' in scheme:\r\n        return 'SOLUTION ORIENTED SCHEME CHILDREN'\r\n    elif 'OTHER SCHEME' in scheme:\r\n        return 'OTHERS'\r\n    elif 'FORMERLY SUPER INSTITUTIONAL PLAN' in scheme:\r\n        return 'FORMERLY SUPER INSTITUTIONAL PLAN'\r\n    elif 'FORMERLY KNOWN AS IIFL MUTUAL FUND' in scheme:\r\n        return 'FORMERLY KNOWN AS IIFL MUTUAL FUND'\r\n    elif any(keyword in scheme for keyword in ['DAYS', 'PLAN', 'DAILY', 'HALF YEARLY', 'ANNUAL', 'COMPULSORY']):\r\n        return 'DURATION/PLAN'\r\n    elif 'DIRECT' in scheme or 'PAYOUT' in scheme:\r\n        return 'DIRECT/PAYOUT'\r\n    elif 'IDF' in scheme or 'GROWTH' in scheme or 'LIQUID' in scheme:\r\n        return 'IDF/GROWTH/LIQUID'\r\n    else:\r\n        return 'MISCELLANEOUS'\r\n        \r\n        \r\n\r\n# Define a function to process each DataFrame\r\ndef process_df(df):\r\n    df = df.drop(['year'], axis=1)\r\n    df = df.groupby(['date', 'nav', 'fund_house', 'scheme_type', 'scheme_category', 'scheme_code', 'scheme_name']).size().reset_index(name='count')\r\n    df = df.drop('count', axis=1)\r\n    df['scheme_category'] = df['scheme_category'].str.upper()\r\n    df['scheme_category'] = df['scheme_category'].apply(categorize4)\r\n    df['scheme_type'] = df['scheme_type'].apply(clean_text)\r\n    df = df.loc[:, ~df.columns.str.contains('^Unnamed')]\r\n    return df\r\n\r\n# Process each DataFrame and update the original variables\r\ncombined_df_2006_2011 = process_df(combined_df_2006_2011)\r\ncombined_df_2012_2015 = process_df(combined_df_2012_2015)\r\ncombined_df_2016_2019 = process_df(combined_df_2016_2019)\r\ncombined_df_2020_2022 = process_df(combined_df_2020_2022)\r\ncombined_df_2023_2024 = process_df(combined_df_2023_2024)\r\n\r\ncombined_df_2006_2011['nav'] = combined_df_2006_2011['nav'].round(2)\r\ncombined_df_2012_2015['nav'] = combined_df_2012_2015['nav'].round(2)\r\ncombined_df_2016_2019['nav'] = combined_df_2016_2019['nav'].round(2)\r\ncombined_df_2020_2022['nav'] = combined_df_2020_2022['nav'].round(2)\r\ncombined_df_2023_2024['nav'] = combined_df_2023_2024['nav'].round(2)\r\n\r\n\r\ns3 = boto3.client('s3')\r\nbucket_name = 'cleaned-capstone-bucket'\r\n\r\n# for 1st file\r\nfile_key1 = 'combined_df_2006_2011_cleaned.csv'\r\n\r\ncsv_buffer = StringIO()\r\ncombined_df_2006_2011.to_csv(csv_buffer, index=False)\r\n\r\n# Upload CSV string to S3\r\ns3.put_object(Bucket=bucket_name, Key=file_key1, Body=csv_buffer.getvalue())\r\n\r\n# for 2nd file\r\n\r\nfile_key2 = 'combined_df_2012_2015_cleaned.csv'\r\n\r\ncsv_buffer = StringIO()\r\ncombined_df_2012_2015.to_csv(csv_buffer, index=False)\r\n\r\n# Upload CSV string to S3\r\ns3.put_object(Bucket=bucket_name, Key=file_key2, Body=csv_buffer.getvalue())\r\n\r\n\r\n# for 3rd file\r\n\r\nfile_key3 = 'combined_df_2016_2019_cleaned.csv'\r\n\r\ncsv_buffer = StringIO()\r\ncombined_df_2016_2019.to_csv(csv_buffer, index=False)\r\n\r\n# Upload CSV string to S3\r\ns3.put_object(Bucket=bucket_name, Key=file_key3, Body=csv_buffer.getvalue())\r\n\r\n\r\n# for 4rth file\r\nfile_key4 = 'combined_df_2020_2022_cleaned.csv'\r\n\r\ncsv_buffer = StringIO()\r\ncombined_df_2020_2022.to_csv(csv_buffer, index=False)\r\n\r\n# Upload CSV string to S3\r\ns3.put_object(Bucket=bucket_name, Key=file_key4, Body=csv_buffer.getvalue())\r\n\r\n\r\n# for 5th file\r\nfile_key5 = 'combined_df_2023_2024_cleaned.csv'\r\n\r\ncsv_buffer = StringIO()\r\ncombined_df_2023_2024.to_csv(csv_buffer, index=False)\r\n\r\n# Upload CSV string to S3\r\ns3.put_object(Bucket=bucket_name, Key=file_key5, Body=csv_buffer.getvalue())\r\n\r\n\r\n\r\n"
}